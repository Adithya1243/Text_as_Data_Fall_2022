{
  "hash": "d826954556b7a28a04f11699d1934945",
  "result": {
    "markdown": "---\ntitle: \"Blog 6 - Conclusions, Scope and Future Analysis\"\nauthor: \"Adithya Parupudi\"\ndesription: \"\"\ndate: \"12/06/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Adithya Parupudi\n---\n\n\n# Libraries\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(stm)\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n# Reading data\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- read_csv(\"./100FamousPeople_new.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNew names:\nRows: 100 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(8): people_names, peoples_title, content, from, to, profession, country... dbl\n(2): ...1, ...2\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n• `...1` -> `...2`\n```\n:::\n\n```{.r .cell-code}\nhead(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 10\n   ...1  ...2 people_names    peopl…¹ content from  to    profe…² country gender\n  <dbl> <dbl> <chr>           <chr>   <chr>   <chr> <chr> <chr>   <chr>   <chr> \n1     1     1 Abraham Lincoln us pre… “With … 1809   1865 politi… america male  \n2     2     2 Adolf Hitler    leader… Adolf … 1889   1945 politi… germany male  \n3     3     3 Albert Einstein german… Born i… 1879   1955 academ… germany male  \n4     4     4 Alfred Hitchco… englis… Sir Al… 4      40   entert… america male  \n5     5     5 Amelia Earhart… aviator Amelia… 1897  1937  others  others  female\n6     6     6 Angelina Jolie  actres… Angeli… 1975  2022  entert… others  female\n# … with abbreviated variable names ¹​peoples_title, ²​profession\n```\n:::\n:::\n\n\n\n\n# Artists\nI am filtering the data set only to see the artists. By running the table command, we can see the distribution of artists in all countries and we can observe that America has the highest number of artists at 5, followed by Europe at 4 and Britain at 3.\n\n::: {.cell}\n\n```{.r .cell-code}\nx<-dataset %>% select(country, people_names, profession) %>% filter(profession == 'artist')\n\ntable(x$country)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\namerica british  europe  france germany \n      5       3       4       1       1 \n```\n:::\n:::\n\n\n# STM on artists\n\nCreating separate variables to identify artists of America, Europe and Britain separately. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_america <- dataset %>% select(country, people_names, profession, content) %>% filter(profession == 'artist' & country == 'america') \n\nartist_british <- dataset %>% select(country, people_names, profession, content) %>% filter(profession == 'artist' & country == 'british') \n\nartist_europe <- dataset %>% select(country, people_names, profession, content) %>% filter(profession == 'artist' & country == 'europe') \n```\n:::\n\n\n\n\n# America artists\n\n## Running topic models\nPerforming data preprocessing by removing stop words, punctuation, converting to lowercase, and stemming words. Using the prepDocuments() function as a base to run an STM with K = 6\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_america_processed <- textProcessor(documents = artist_america$content, \n                           metadata = artist_america,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\nartist_america_prepped <- prepDocuments(documents = artist_america_processed$documents,\n                         vocab = artist_america_processed$vocab,\n                         meta = artist_america_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 993 of 1131 terms (1193 of 1674 tokens) due to frequency \nYour corpus now has 5 documents, 138 terms and 481 tokens.\n```\n:::\n\n```{.r .cell-code}\nartist_america_basicmodel <- stm(documents = artist_america_prepped$documents,\n                         vocab = artist_america_prepped$vocab,\n                         data = artist_america_prepped$meta,\n                         K=6,\n                         verbose = F)\n```\n:::\n\n\n## Plot Topics\nWe can observe 6 topics below\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.STM(artist_america_basicmodel)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n## Topic correlation\nWe can observe that topics 1,5,6 are correlated\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(topicCorr(artist_america_basicmodel))\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Wordcloud of topic 1\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = artist_america_basicmodel,\n      topic=1,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n## Wordcloud of topic 5\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = artist_america_basicmodel,\n      topic=5,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Wordcloud of topic 6\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = artist_america_basicmodel,\n      topic=6,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n## Interpretation\nWith K=5, picked topic with highest relevance to the documents. All of them were musicians(jazz, pop, rock n roll etc). Sales, happiness, record, career -> talk about their success. These artists come from a humble background. Divorce, death, break -> negatives which speak about their challenges in life. Either lost someone close to them, or they have expired at a young age.\n\n\n\n# Britain artists\nPerforming data preprocessing by removing stop words, punctuation, converting to lowercase, and stemming words. Using the prepDocuments() function as a base to run an STM with K = 3. There is lack of more information, hence cannot observe any correlations. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_british_processed <- textProcessor(documents = artist_british$content, \n                           metadata = artist_british,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\nartist_british_prepped <- prepDocuments(documents = artist_british_processed$documents,\n                         vocab = artist_british_processed$vocab,\n                         meta = artist_british_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 698 of 740 terms (841 of 967 tokens) due to frequency \nYour corpus now has 3 documents, 42 terms and 126 tokens.\n```\n:::\n\n```{.r .cell-code}\nartist_british_basicmodel <- stm(documents = artist_british_prepped$documents,\n                         vocab = artist_british_prepped$vocab,\n                         data = artist_british_prepped$meta,\n                         K=3,\n                         verbose = F)\n```\n:::\n\n\n## Plotting STM\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.STM(artist_british_basicmodel)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Topic correlation\nLooks like there is no correlation between 3 topics. This is due to unavailability of more data. If there was more data ( referring to more number of artists in Britain), then there is a possibility of observing more relations\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(topicCorr(artist_british_basicmodel))\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Word cloud of topic 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = artist_british_basicmodel,\n      topic=1,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n## Interpretation\nHad 3 artists. With k=3, this is also focusing on the Beatles musical career and their success in the music industry. As you can see there is not much correlation between the topics. John Lennon took a solo career path and gained huge fame.\n\n\n# European artists\nPerforming data preprocessing by removing stop words, punctuation, converting to lowercase, and stemming words. Using the prepDocuments() function as a base to run an STM with K = 5\n\n::: {.cell}\n\n```{.r .cell-code}\nartist_europe_processed <- textProcessor(documents = artist_europe$content, \n                           metadata = artist_europe,\n                           lowercase = T,\n                           removestopwords = T,\n                           removenumbers = T,\n                           removepunctuation = T,\n                           stem=T,\n                           wordLengths = c(3,Inf),\n                           language = \"en\",\n                           onlycharacter = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n```\n:::\n\n```{.r .cell-code}\nartist_europe_prepped <- prepDocuments(documents = artist_europe_processed$documents,\n                         vocab = artist_europe_processed$vocab,\n                         meta = artist_europe_processed$meta,\n                         lower.thresh = 2,\n                         upper.thresh = Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoving 1065 of 1137 terms (1234 of 1463 tokens) due to frequency \nYour corpus now has 4 documents, 72 terms and 229 tokens.\n```\n:::\n\n```{.r .cell-code}\nartist_europe_basicmodel <- stm(documents = artist_europe_prepped$documents,\n                         vocab = artist_europe_prepped$vocab,\n                         data = artist_europe_prepped$meta,\n                         K=5,\n                         verbose = F)\n\n\nplot.STM(artist_europe_basicmodel)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n## Plot correlated topic model\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(topicCorr(artist_europe_basicmodel))\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Word cloud of topic 1\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = artist_europe_basicmodel,\n      topic=1,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n## Word cloud of topic 5\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncloud(stmobj = artist_europe_basicmodel,\n      topic=5,\n      random.order=F,\n      rot.per=0)\n```\n\n::: {.cell-output-display}\n![](blogpost6_AdithyaParupudi_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Interpretation\nTheir interpretation revolved around money, world and work. It started out as a dark past, with family issues and later observed success. \n\n\n# Conclusions\n\nSince the world witnessed two world wars in the past century (with tensions still existing today), it was interesting to see the inception of a new era during the warring period where many leaders stood up to what they believed in, and let to the foundations of a nation. There were many mentions of internal and external battles in each politician’s life. In other categories, people had many humble beginnings in their sports and music career, where they later branched out to have their own style. But commonly, war played a significant roles in shaping their career.\n\n\nFor artists, All of them were musicians(jazz, pop, rock n roll etc). Sales, happiness, record, career -> talk about their success. These artists come from a humble background. Divorce, death, break -> negatives which speak about their challenges in life. Either lost someone close to them, or they have expired at a young age.\n\n\nFor academicians\nStories and struggles of people like J.K Rowling, Charles Darwin, C.S Lewis are shown in these results. Their life stories show passion, setbacks and growth on their literary works. Though they faced a lot of criticism, their works were became popular after they passed away. \n\n# Limitations\n\n\n\nThis project had many modules, and I was not able to interpret categories such as spiritual, royalty and humanitarian. Firstly, because there is not much information in their biographies, i.e., there is not enough data that covers all aspects of their life. Secondly there are not enough people to find similarities. From my research, I understood that results will be much interpretable if I have a larger dataset to work with, for unsupervised learning.\n\nWhile I am trying to use searchK() to find the optimal k value, I am getting an error. I have tweaked this function in multiple ways but there was no result. From what I understood through experimenting is, searchK() only works when the data set is high in number. \n\nSince I filtered the dataset(using multiple filters), each scenario has utmost 4 rows worth of information which is too low for searchK to provide any insights on this. \n\n\n\n# Future Scope\nFor further analysis, it would be interesting to see the leadership traits between people of the same profession, belonging to different countries. There is also a scope to perform sentiment analysis to find out what people actually think of their work. A future research question would be ‘Were people really as popular as they are perceived?’ Comments on leaders perspective through tweets, news articles, comment section, polls etc make a good dataset\n",
    "supporting": [
      "blogpost6_AdithyaParupudi_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}